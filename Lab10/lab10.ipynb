{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,p1,p2):\n",
    "        self.board = np.zeros((3,3))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.current_player = 1 #1 is p1, -1 is p2\n",
    "\n",
    "    def available_positions(self):\n",
    "        pos = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i,j] == 0:\n",
    "                    pos.append((i,j))\n",
    "        return pos\n",
    "    \n",
    "    def make_move(self, position):\n",
    "        if position not in self.available_positions():\n",
    "            return None\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player = self.current_player*-1\n",
    "\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(3 * 3))\n",
    "        return self.boardHash\n",
    "\n",
    "    def check_winner(self):\n",
    "        #check if rows contains 3 or -3 (some one win)\n",
    "        for i in range(3): \n",
    "            if sum(self.board[i,:]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1 #player 1 won\n",
    "        for i in range(3): #loop on the rows\n",
    "            if sum(self.board[i,:]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1 #player 2 won\n",
    "        \n",
    "        #check if col contains 3 or -3\n",
    "        for i in range(3):\n",
    "            if sum(self.board[:,i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "        for i in range(3):\n",
    "            if sum(self.board[:,i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        \n",
    "        #check diagonal win\n",
    "        diag_sum = sum([self.board[i,i] for i in range(3)])\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd= True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        diag_sum = sum([self.board[i,3-i-1] for i in range(3)])\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd= True\n",
    "            return 1\n",
    "        if diag_sum == -3:\n",
    "            self.isEnd = True\n",
    "            return -1\n",
    "        \n",
    "        #here no one won..\n",
    "        if len(self.available_positions())==0 :\n",
    "            self.isEnd = True\n",
    "            return 0 #no one won\n",
    "        \n",
    "        return None #Here there are still moves, so keep playing !!!\n",
    "    \n",
    "    def reward(self):\n",
    "        result = self.check_winner()\n",
    "\n",
    "        if result == 1:\n",
    "            self.p1.give_rew(1) #player 1 won, so give 1 reward\n",
    "            self.p2.give_rew(0)\n",
    "        elif result == -1:\n",
    "            self.p1.give_rew(0)\n",
    "            self.p2.give_rew(1)\n",
    "        else:\n",
    "            self.p1.give_rew(0.1) #give a less reward because we don't want ties\n",
    "            self.p2.give_rew(0.5)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, 3):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, 3):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')    \n",
    "\n",
    "    def train(self, rounds=100):\n",
    "        for i in range(rounds):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.available_positions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.current_player)\n",
    "                # take action and upate board state\n",
    "                self.make_move(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.check_winner()\n",
    "                if win is not None: #It returns None only when no one finished or tied.\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.reward() #send rewards to the players, the game has ended\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.available_positions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.current_player)\n",
    "                    self.make_move(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "\n",
    "                    win = self.check_winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.reward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "\n",
    "    def test(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.available_positions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.current_player)\n",
    "            # take action and upate board state\n",
    "            self.make_move(p1_action)\n",
    "            #self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.check_winner()\n",
    "            if win is not None: #if win not None means some one win or tie\n",
    "                return win\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.available_positions()\n",
    "                p2_action = self.p2.chooseAction(positions, self.board, self.current_player)\n",
    "\n",
    "                self.make_move(p2_action)\n",
    "                #self.showBoard()\n",
    "                win = self.check_winner()\n",
    "                if win is not None:\n",
    "                    return win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLPlayer:\n",
    "    def __init__(self, name, exp_rate = 0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "\n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(3*3))\n",
    "        return boardHash\n",
    "\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        \"\"\"Return a random action (P = 0.3) or the action with max value (P = 0.7)\"\"\"\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate: # Do exploration, take random \n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else: #Here do exploitation, take the action that has highest value\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy() #create a tmp board\n",
    "                next_board[p] = symbol #do the action\n",
    "                next_boardHash = self.getHash(next_board) #get the hash\n",
    "                value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max: #find the action that has max value. \n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def give_rew(self, reward):\n",
    "        #At the end of the game i'll get a reward. The iterate on the states in reverse.\n",
    "        # set the value of the state to 0 if not existing, otherwise update it with the reward. \n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None: #if the state doesn't have a value, set it to 0\n",
    "                self.states_value[st] = 0\n",
    "            #this is V(t) = V(t) + lr * (gamma*V(t+1) - V(t))\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name \n",
    "    \n",
    "    def chooseAction(self, positions,current_board, symbol):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "    \n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "    \n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def give_rew(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer:\n",
    "    def __init__(self, name):\n",
    "        self.name = \"random\"\n",
    "\n",
    "    def chooseAction(self, positions,current_board, symbol):\n",
    "        x = np.random.randint(0,len(positions)-1)\n",
    "        return positions[x]\n",
    "    \n",
    "    def addState(self,state):\n",
    "        pass\n",
    "\n",
    "    def give_rew(self, reward):\n",
    "        pass\n",
    "            \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def give_rew(self,rew):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLPlayer:\n",
    "    def __init__(self, name, player, exp_rate = 0.3):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken + action\n",
    "        self.actions = [\"TL\",\"TM\",\"TR\",\"ML\",\"MM\",\"MR\",\"BL\",\"MB\",\"BR\"]\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = exp_rate\n",
    "        self.decay_gamma = 0.9\n",
    "\n",
    "        self.Q_values = {}\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                self.Q_values[(i, j)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.Q_values[(i, j)][a] = 0  # Q value is a dict of dict\n",
    "\n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(3*3))\n",
    "        return boardHash\n",
    "\n",
    "    def addState(self, state, action):\n",
    "        self.states.append((state,action))\n",
    "\n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        \"\"\"Return a random action (P = 0.3) or the action with max value (P = 0.7)\"\"\"\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate: # Do exploration, take random \n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else: #Here do exploitation, take the action that has highest value\n",
    "            value = 0\n",
    "            for p in positions:\n",
    "                for a in self.actions:\n",
    "                    next_value = self.Q_values[p][a]\n",
    "                    if next_value > value:\n",
    "                        value = next_value\n",
    "                        action = a\n",
    "                        position = p\n",
    "        return action, position\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def give_rew(self, reward):\n",
    "        #At the end of the game i'll get a reward. The iterate on the states in reverse.\n",
    "        # set the value of the state to 0 if not existing, otherwise update it with the reward. \n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None: #if the state doesn't have a value, set it to 0\n",
    "                self.states_value[st] = 0\n",
    "            #this is V(t) = V(t) + lr * (gamma*V(t+1) - V(t))\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds 0\n",
      "Rounds 1000\n",
      "Rounds 2000\n",
      "Rounds 3000\n",
      "Rounds 4000\n",
      "Rounds 5000\n",
      "Rounds 6000\n",
      "Rounds 7000\n",
      "Rounds 8000\n",
      "Rounds 9000\n",
      "Rounds 10000\n",
      "Rounds 11000\n",
      "Rounds 12000\n",
      "Rounds 13000\n",
      "Rounds 14000\n",
      "Rounds 15000\n",
      "Rounds 16000\n",
      "Rounds 17000\n",
      "Rounds 18000\n",
      "Rounds 19000\n",
      "Rounds 20000\n",
      "Rounds 21000\n",
      "Rounds 22000\n",
      "Rounds 23000\n",
      "Rounds 24000\n",
      "Rounds 25000\n",
      "Rounds 26000\n",
      "Rounds 27000\n",
      "Rounds 28000\n",
      "Rounds 29000\n",
      "Rounds 30000\n",
      "Rounds 31000\n",
      "Rounds 32000\n",
      "Rounds 33000\n",
      "Rounds 34000\n",
      "Rounds 35000\n",
      "Rounds 36000\n",
      "Rounds 37000\n",
      "Rounds 38000\n",
      "Rounds 39000\n",
      "Rounds 40000\n",
      "Rounds 41000\n",
      "Rounds 42000\n",
      "Rounds 43000\n",
      "Rounds 44000\n",
      "Rounds 45000\n",
      "Rounds 46000\n",
      "Rounds 47000\n",
      "Rounds 48000\n",
      "Rounds 49000\n",
      "Rounds 50000\n",
      "Rounds 51000\n",
      "Rounds 52000\n",
      "Rounds 53000\n",
      "Rounds 54000\n",
      "Rounds 55000\n",
      "Rounds 56000\n",
      "Rounds 57000\n",
      "Rounds 58000\n",
      "Rounds 59000\n",
      "Rounds 60000\n",
      "Rounds 61000\n",
      "Rounds 62000\n",
      "Rounds 63000\n",
      "Rounds 64000\n",
      "Rounds 65000\n",
      "Rounds 66000\n",
      "Rounds 67000\n",
      "Rounds 68000\n",
      "Rounds 69000\n",
      "Rounds 70000\n",
      "Rounds 71000\n",
      "Rounds 72000\n",
      "Rounds 73000\n",
      "Rounds 74000\n",
      "Rounds 75000\n",
      "Rounds 76000\n",
      "Rounds 77000\n",
      "Rounds 78000\n",
      "Rounds 79000\n",
      "Rounds 80000\n",
      "Rounds 81000\n",
      "Rounds 82000\n",
      "Rounds 83000\n",
      "Rounds 84000\n",
      "Rounds 85000\n",
      "Rounds 86000\n",
      "Rounds 87000\n",
      "Rounds 88000\n",
      "Rounds 89000\n",
      "Rounds 90000\n",
      "Rounds 91000\n",
      "Rounds 92000\n",
      "Rounds 93000\n",
      "Rounds 94000\n",
      "Rounds 95000\n",
      "Rounds 96000\n",
      "Rounds 97000\n",
      "Rounds 98000\n",
      "Rounds 99000\n"
     ]
    }
   ],
   "source": [
    "p1 = RLPlayer(\"computer\")\n",
    "p2 = RLPlayer(\"computer\")\n",
    "\n",
    "st = State(p1,p2)\n",
    "st.train(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = RandomPlayer(\"Random\")\n",
    "epochs = 1000\n",
    "st = State(p1,p2)\n",
    "win_comp = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    win = st.test()\n",
    "    if win == 1:\n",
    "        win_comp+=1 \n",
    "    st.reset()\n",
    "\n",
    "perc = win_comp / epochs * 100\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n",
      "|   | x |   | \n",
      "-------------\n",
      "|   |   |   | \n",
      "-------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m p2 \u001b[38;5;241m=\u001b[39m HumanPlayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m st \u001b[38;5;241m=\u001b[39m State(p1, p2)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 170\u001b[0m, in \u001b[0;36mState.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Player 2\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_positions()\n\u001b[0;32m--> 170\u001b[0m     p2_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_player\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_move(p2_action)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshowBoard()\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mHumanPlayer.chooseAction\u001b[0;34m(self, positions, current_board, symbol)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchooseAction\u001b[39m(\u001b[38;5;28mself\u001b[39m, positions,current_board, symbol):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m         row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput your action row:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput your action col:\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m         action \u001b[38;5;241m=\u001b[39m (row, col)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
